import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans



#Reading data from csv files
df_input=pd.read_csv("/home/ashwin/Downloads/Querylevelnorm_X.csv", sep=',',header=None)
df_output=pd.read_csv("/home/ashwin/Downloads/Querylevelnorm_t.csv", sep=',',header=None)
df_output.columns = ['y']
df = pd.concat([df_input, df_output] , axis =1 )
train, validate, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])
#Creating train sets
train_input =  train[train.columns[0:46]]
train_output = train['y']
trainsize, _ = train_input.shape
#Creating validation sets
validate_input =  validate[validate.columns[0:46]]
validate_output = validate['y']
#Creating test sets
test_input =  test[test.columns[0:46]]
test_output = test['y']



#Initializing input and output data
input_data = np.array(train_input)

#output_data = np.array(train_output)
output_data = train_output.reshape([-1,1])
print("Output_data shape:" , output_data.shape)


kmeans = KMeans(n_clusters=3)
KOut = kmeans.fit(train_input)
KOut.cluster_centers_

def compute_design_matrix(X, centers, spreads):
 # use broadcast 
 basis_func_outputs = np.exp(np.sum(np.matmul(X - centers, spreads) * (X - centers),axis=2) / (-2)).T
 # insert ones to the 1st col
 return np.insert(basis_func_outputs, 0, 1, axis=1)

def calculate_centers():
  
   return np.array(KOut.cluster_centers_)  
    
def calculate_spreads():
    diagonal = np.zeros(shape = (46,46))
    k=5 
    for i in range(0,46) : 
        val = np.var(input_data[:,i])
        diagonal[i,i]= val * k
    return np.array([np.array(diagonal),np.array(diagonal),np.array(diagonal)])

def closed_form_sol(L2_lambda, design_matrix, output_data):
    return np.linalg.solve(L2_lambda * np.identity(design_matrix.shape[1]) + np.matmul(design_matrix.T, design_matrix), np.matmul(design_matrix.T, output_data)).flatten()

def SGD_sol(learning_rate, minibatch_size, num_epochs, L2_lambda,design_matrix,output_data):
    N, _ = design_matrix.shape
    # You can try different mini-batch size size
    # Using minibatch_size = 1 is equivalent to standard gradient descent
    # Using minibatch_size = N is equivalent to stochastic gradient descent
    # In this case, minibatch_size = N is better
    weights = np.random.randn(1, 4) * 0.1
    # The more epochs the higher training accuracy. When set to 1000000,
    # weights will be very close to closed_form_weights. But this is unnecessary
    lst = []
    for epoch in range(num_epochs):
      
      for i in range(int(N / minibatch_size)):
         lower_bound = i * minibatch_size
         upper_bound = min((i+1)*minibatch_size, N)
         Phi = design_matrix[lower_bound : upper_bound, :]
         t = output_data[lower_bound : upper_bound , :]
         E_D = np.matmul((np.matmul(Phi, weights.T)-t).T,Phi )
         E = (E_D + L2_lambda * weights) / minibatch_size
         weights = weights - learning_rate * E
      
      lst.append(np.linalg.norm(E))            
    
    return weights.flatten() , lst   


N , D = input_data.shape 

# shape = [1, N, D]
X = input_data[np.newaxis, :, :]
centers = calculate_centers() 
# M * 1 * D
centers = centers[:, np.newaxis, :]
spreads = calculate_spreads()

# M * D * D
design_matrix = compute_design_matrix(X, centers, spreads)

# Closed-form solution
print("Closed form" ,closed_form_sol(L2_lambda=0.1,design_matrix=design_matrix,output_data=output_data))
    

# Gradient descent solution

Z, Y = SGD_sol(learning_rate=0.03,minibatch_size=256,num_epochs=1000,L2_lambda=0.4,design_matrix=design_matrix,output_data=output_data)    
%matplotlib inline
print("Gradient decent solution" , Z)
plt.plot(range(len(Y)),Y)

   
